{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BKxEYRMjZaLT"
   },
   "source": [
    "# Step 1. Project Description\n",
    "In this project, we will use the standard machine-learning problem called the iris flowers dataset:\n",
    "\n",
    "http://archive.ics.uci.edu/ml/datasets/Iris \n",
    "\n",
    "This dataset is well studied and is a good problem for practicing on neural networks because all of the 4 input variables are numeric and have the same scale in centimeters. Each instance describes the properties of an observed flower measurements and the output variable is specific iris species.\n",
    "\n",
    "This is a multi-class classification problem, meaning that there are more than two classes to be predicted, in fact there are three flower species. This is an important type of problem on which to practice with neural networks because the three class values require specialized handling.\n",
    "\n",
    "The iris flower dataset is a well-studied problem and as such we can expect to achieve model accuracy in the range of 95% to 97%. This provides a good target to aim for when developing our models in this project.\n",
    "\n",
    "We have download the iris flowers dataset for free and place it in the project directory with the filename “iris.csv“. You can also directly download the dataset:\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eJm7RqdoZery"
   },
   "source": [
    "# Step 2. Making Preparations\n",
    "We will start off by importing all of the classes and functions we will need. This includes both the functionality we require from Keras, but also data loading from pandas as well as data preparation and model evaluation from scikit-learn:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HAKucf2qZhja"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.utils import np_utils\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JOrqRPiTaFaB"
   },
   "source": [
    "Next, we need to initialize the random number generator to a constant value (7).\n",
    "\n",
    "This is important to ensure that the results we achieve from this model can be achieved again precisely. It ensures that the stochastic process of training a neural network model can be reproduced:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UKZDFnnFaGJL"
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ANzwe07FaIoa"
   },
   "source": [
    "The dataset can be loaded directly. Because the output variable contains strings, it is easiest to load the data using pandas. We can then split the attributes (columns) into input variables (x) and output variables (y)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qTdHvZPvaN1L"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3\n",
       "count  150.000000  150.000000  150.000000  150.000000\n",
       "mean     5.843333    3.054000    3.758667    1.198667\n",
       "std      0.828066    0.433594    1.764420    0.763161\n",
       "min      4.300000    2.000000    1.000000    0.100000\n",
       "25%      5.100000    2.800000    1.600000    0.300000\n",
       "50%      5.800000    3.000000    4.350000    1.300000\n",
       "75%      6.400000    3.300000    5.100000    1.800000\n",
       "max      7.900000    4.400000    6.900000    2.500000"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "dataframe = pd.read_csv(\"iris.csv\", header=None)\n",
    "dataframe.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalized  = dataframe.copy()\n",
    "# normalized_data = normalized.iloc[:,0:4]\n",
    "# normalized_target = normalized.iloc[:,4:]\n",
    "# normalized_data = normalized_data/normalized_data.max(axis=0)\n",
    "# normalized = pd.concat([normalized_data, normalized_target], axis = 1)\n",
    "# dataset = normalized.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values\n",
    "x = dataset[:,0:4].astype(float)\n",
    "y = dataset[:,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IUXntBY3ad7K"
   },
   "source": [
    "The output variable contains three different string values.\n",
    "<p>\n",
    "When modeling multi-class classification problems using neural networks, it is good practice to reshape the output attribute from a vector that contains values for each class value to be a matrix with a boolean for each class value and whether or not a given instance has that class value or not.\n",
    "<p/>\n",
    "<p>\n",
    "This is called one hot encoding or creating dummy variables from a categorical variable:\n",
    "https://en.wikipedia.org/wiki/One-hot\n",
    "<p/>\n",
    "<p>\n",
    "For example, in this problem three class values are Iris-setosa, Iris-versicolor and Iris-virginica. If we had the observations:\n",
    "<p/>\n",
    "<p>\n",
    "Iris-setosa\n",
    "</p>\n",
    "Iris-versicolor\n",
    "\n",
    "Iris-virginica\n",
    "\n",
    "We can turn this into a one-hot encoded binary matrix for each data instance that would look as follows:\n",
    "\n",
    "Iris-setosa,\tIris-versicolor,\t              Iris-virginica\n",
    "1,\t\t0,\t\t\t0\n",
    "0,\t\t1, \t\t\t0\n",
    "0, \t\t0, \t\t\t1\n",
    "\n",
    "\n",
    "We can do this by first encoding the strings consistently to integers using the scikit-learn class LabelEncoder. Then convert the vector of integers to a one hot encoding using the Keras function to_categorical().\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YAZjsxedbCny"
   },
   "outputs": [],
   "source": [
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y_encoded = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(y_encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YqYiyMr9bJwy"
   },
   "source": [
    "Step 3: Define the Neural Network Baseline Model\n",
    "The Keras library provides wrapper classes to allow you to use neural network models developed with Keras in scikit-learn.\n",
    "\n",
    "There is a KerasClassifier class in Keras that can be used as an Estimator in scikit-learn, the base type of model in the library. The KerasClassifier takes the name of a function as an argument. This function must return the constructed neural network model, ready for training.\n",
    "\n",
    "Below is a function that will create a baseline neural network for the iris classification problem. It creates a simple fully connected network with one hidden layer that contains 8 neurons.\n",
    "\n",
    "The hidden layer uses a rectifier activation function, which is a good practice. Because we used a one-hot encoding for our iris dataset, the output layer must create 3 output values, one for each class. The output value with the largest value will be taken as the class predicted by the model.\n",
    "\n",
    "The network topology of this simple one-layer neural network can be summarized as:\n",
    "\n",
    "\n",
    "4 inputs -> [8 hidden nodes] -> 3 outputs\n",
    "\n",
    "Note that we use a “softmax” activation function in the output layer. This is to ensure the output values are in the range of 0 and 1 and may be used as predicted probabilities.\n",
    "\n",
    "Finally, the network uses the efficient Adam gradient descent optimization algorithm with a logarithmic loss function, which is called “categorical_crossentropy” in Keras.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qzIZOpB2bEfy"
   },
   "outputs": [],
   "source": [
    "# define baseline model\n",
    "def baseline_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ebs9XYeKbMhS"
   },
   "source": [
    "We can now create our KerasClassifier for use in scikit-learn.\n",
    "\n",
    "We can also pass arguments in the construction of the KerasClassifier class that will be passed on to the fit() function internally used to train the neural network. Here, we pass the number of epochs as 200 and batch size as 5 to use when training the model. Debugging is also turned off when training by setting verbose to 0.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hxo-mlZubTNi"
   },
   "outputs": [],
   "source": [
    "estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S-QFkMMFbVDZ"
   },
   "source": [
    "# Step 4. Evaluate The Model with k-Fold Cross Validation\n",
    "We can now evaluate the neural network model on our training data.\n",
    "\n",
    "The scikit-learn has excellent capability to evaluate models using a suite of techniques. The gold standard for evaluating machine learning models is k-fold cross validation.\n",
    "\n",
    "First we can define the model evaluation procedure. Here, we set the number of folds to be 10 (an excellent default) and to shuffle the data before partitioning it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zFnlpIODbaaz"
   },
   "outputs": [],
   "source": [
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "y1BCIqSbbf06"
   },
   "source": [
    "Now we can evaluate our model (estimator) on our dataset (X and dummy_y) using a 10-fold cross-validation procedure (kfold).\n",
    "\n",
    "Evaluating the model only takes approximately 10 seconds and returns an object that describes the evaluation of the 10 constructed models for each of the splits of the dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "QIDlkG4xbcJL",
    "outputId": "5614ec91-4b84-4cd2-943e-da4def6a4828"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bPvdulE3bjKi"
   },
   "source": [
    "The results are summarized as both the mean and standard deviation of the model accuracy on the dataset. This is a reasonable estimation of the performance of the model on unseen data. It is also within the realm of known top results for this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_SqtJ8qpbuHZ"
   },
   "source": [
    "# Step 5. Tuning Layers and Number of Neurons in The Model\n",
    "There are many things to tune on a neural network, such as the weight initialization, activation functions, optimization procedure and so on.\n",
    "\n",
    "One aspect that may have an outsized effect is the structure of the network itself called the network topology. In this section, we take a look at two experiments on the structure of the network: making it smaller and making it larger.\n",
    "\n",
    "These are good experiments to perform when tuning a neural network on your problem.\n",
    "\n",
    "\n",
    "## Step 5.1. Evaluate a Smaller Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Rkbr6DM8lIYZ",
    "outputId": "e2219f9b-307e-4cf9-a505-864b291fd401"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 82.67% (24.07%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# define small model\n",
    "def small_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(4, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=small_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kmLqn6uUbwry"
   },
   "source": [
    "## Step 5.2. Evaluate a Larger Network\n",
    "A neural network topology with more layers offers more opportunity for the network to extract key features and recombine them in useful nonlinear ways.\n",
    "\n",
    "We can evaluate whether adding more layers to the network improves the performance easily by making another small tweak to the function used to create our model. \n",
    "\n",
    "The idea here is that the network is given the opportunity to model all input variables before being bottlenecked and forced to halve the representational capacity, much like we did in the experiment above with the smaller network.\n",
    "\n",
    "Instead of squeezing the representation of the inputs themselves, we add hidden layers to aid in the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "iEq8cEZ5liLX",
    "outputId": "64f04019-d5ca-4965-f24d-e2b44d316889",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# define large model\n",
    "def large_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(16, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(6, activation = 'relu'))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=large_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "upkDUrHLbzyZ"
   },
   "source": [
    "# Step 6. Really Scaling up: developing a model that overfits\n",
    "Once you’ve obtained a model that has statistical power, the question becomes, is your\n",
    "model sufficiently powerful? Does it have enough layers and parameters to properly\n",
    "model the problem at hand? \n",
    "\n",
    "Remember that the universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
    "\n",
    "To figure out how big a model you’ll need, you must develop a model that overfits.\n",
    "This is fairly easy:\n",
    "1.\tAdd layers.\n",
    "2.\tMake the layers bigger.\n",
    "3.\tTrain for more epochs.\n",
    "\n",
    "Always monitor the training loss and validation loss, as well as the training and validation values for any metrics you care about. When you see that the model’s performance on the validation data begins to degrade, you’ve achieved overfitting.\n",
    "\n",
    "The next step is to start regularizing and tuning the model, to get as close as possible to the ideal model that neither underfits nor overfits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_Fu9oQhcmq_-",
    "outputId": "86d8acf8-ff7b-4b65-e79b-a1797eb9238f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 96.67% (4.47%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# define large model\n",
    "def large_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(30, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(20, activation = 'relu',))\n",
    "  model.add(layers.Dense(10, activation = 'relu',))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=large_model, epochs=250, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o1bPIKUXcIuy"
   },
   "source": [
    "# Step 7. Tuning the Model\n",
    "With further tuning of aspects like the optimization algorithm etc. and the number of training epochs, it is expected that further improvements are possible. What is the best score that you can achieve on this dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "IEy4hOOHm5Te",
    "outputId": "7649f6a7-9aa9-48e9-e809-6524e6602447"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 95.33% (5.21%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# running the same small model with 100 epocs\n",
    "def model_v1():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v1, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# running the small model with 200 epocs\n",
    "def model_v2():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v2, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 96.00% (5.33%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# running the small model with rmsprop\n",
    "def model_v3():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v3, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pPW31eOMFogr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 93.33% (9.89%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# again model with adam but with 6 neurons in hidden layer\n",
    "def model_v4():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(6, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v4, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-NvLxvxsJfth"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 82.67% (24.07%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# again model with adam but with 4 neurons in hidden layer\n",
    "def model_v5():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(4, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v5, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w4IwZQVQPWaz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 91.33% (13.68%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# introducing dropout of 20%\n",
    "def model_v6():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(4, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v6, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5BHMbYfYQUDR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 92.67% (10.09%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# Using regularizers l1\n",
    "from keras import regularizers\n",
    "\n",
    "def model_v7():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(4, activation = 'relu',kernel_regularizer = regularizers.l1(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v7, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 84.67% (21.51%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# Using regularizers l2\n",
    "from keras import regularizers\n",
    "\n",
    "def model_v8():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(4, activation = 'relu',kernel_regularizer = regularizers.l2(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v8, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 95.33% (4.27%)\n"
     ]
    }
   ],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "# Using L1 Regularizer and standarizing the dataset\n",
    "from keras import regularizers\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def model_v7():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(4, activation = 'relu',kernel_regularizer = regularizers.l1(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v7, epochs=200, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 94.00% (5.54%)\n"
     ]
    }
   ],
   "source": [
    "# Using model_v2 again with standarized data set\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v8():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v8, epochs=200, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 94.00% (5.54%)\n"
     ]
    }
   ],
   "source": [
    "# Using model_v2 again with standarized data set and L1 regularizer\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v9():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu',kernel_regularizer = regularizers.l1(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v9, epochs=200, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 96.67% (4.47%)\n"
     ]
    }
   ],
   "source": [
    "# Using model_v2 again with L1 regularizer\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v10():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu',kernel_regularizer = regularizers.l1(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v10, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "# Using model_v2 again with L2 regularizer\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v11():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu',kernel_regularizer = regularizers.l2(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v11, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 96.00% (5.33%)\n"
     ]
    }
   ],
   "source": [
    "# Using rmsprop optimizer\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v12():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(8, activation = 'relu',kernel_regularizer = regularizers.l2(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'rmsprop', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v12, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 98.00% (3.06%)\n"
     ]
    }
   ],
   "source": [
    "# Using adam optimizer and different architecture\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v13():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(10, activation = 'relu',kernel_regularizer = regularizers.l2(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v13, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 97.33% (4.42%)\n"
     ]
    }
   ],
   "source": [
    "# Using different architecture\n",
    "from keras import regularizers\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v14():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(12, activation = 'relu',kernel_regularizer = regularizers.l2(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v14, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 96.67% (5.37%)\n"
     ]
    }
   ],
   "source": [
    "#Using model_v13 with dropout\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v15():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(10, activation = 'relu', input_shape = x[1].shape))\n",
    "  model.add(layers.Dropout(0.2))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "\n",
    "estimator = KerasClassifier(build_fn=model_v15, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So model v_13 is the best one we got so far so we will go with that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ac3aj7kycLqb"
   },
   "source": [
    "# Step 8. Rewriting the code using the Keras Functional API\n",
    "Review the April 9, 2018 presentation done by Chollet contained in the project file: \n",
    "\n",
    "Francois_Chollet_March9.pdf\n",
    "\n",
    "Now rewrite the code that you have written so far using the Keras Sequential API in Kearas Functional API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P1sHvViVahbD"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 98.00% (3.06%)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "\n",
    "def create_model():\n",
    "  \n",
    "  inputs = keras.Input(shape=x[1].shape)\n",
    "  x_1 = layers.Dense(10,activation = 'relu',kernel_regularizer = regularizers.l2(0.001))(inputs)\n",
    "  outputs = layers.Dense(3, activation = 'softmax')(x_1)\n",
    "  model = keras.Model(inputs, outputs)\n",
    "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics=['acc'] )\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gBtUwpRecOVa"
   },
   "source": [
    "# Step 9. Rewriting the code by doing Model Subclassing\n",
    "Now rewrite the code that you have written so far using the Keras Model Subclassing as mentioned in the Chollet April 9, 2018 presentation.\n",
    "\n",
    "Reference:\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "Please note you will have to use TensorFlow 1.7+ with built-in Keras. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline: 98.00% (3.06%)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "class MyModel(keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.dense1 = layers.Dense(10, activation = 'relu', kernel_regularizer = regularizers.l2(0.001))\n",
    "    self.dense2 = layers.Dense(3, activation = 'softmax')\n",
    "   \n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x_1 = self.dense1(inputs)\n",
    "    return self.dense2(x_1)\n",
    "  \n",
    "def create_model():\n",
    "  model = MyModel()\n",
    "  model.compile(optimizer = 'adam',loss = 'categorical_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "\n",
    "estimator = KerasClassifier(build_fn=create_model, epochs=200, batch_size=5, verbose=0)\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, dummy_y, cv=kfold)\n",
    "print(\"Baseline: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aLjsudTycRI6"
   },
   "source": [
    "# Step 10. Rewriting the code without using scikit-learn\n",
    "Once you have written the model in all three API style you are required to do k-fold cross validation without using scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataframe.values\n",
    "np.random.shuffle(dataset)\n",
    "x = dataset[:,0:4].astype(float)\n",
    "y = dataset[:,4]\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "y_encoded = encoder.transform(y)\n",
    "# convert integers to dummy variables (i.e. one hot encoded)\n",
    "dummy_y = np_utils.to_categorical(y_encoded)\n",
    "\n",
    "x1 = x.copy()\n",
    "y1 = dummy_y.copy()\n",
    "x_test = x1[140:]\n",
    "y_test = y1[140:]\n",
    "x_data = x1[:140]\n",
    "y_data = y1[:140]\n",
    "# x_data = x_data-x_data.mean(axis=0)\n",
    "# x_data = x_data/x_data.std(axis=0)\n",
    "# x_test = x_test-x_data.mean(axis=0)\n",
    "# x_test = x_test/x_data.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n",
      "Processing field # 0\n",
      "Processing field # 1\n",
      "Processing field # 2\n",
      "Processing field # 3\n",
      "Processing field # 4\n",
      "Processing field # 5\n",
      "Processing field # 6\n",
      "Processing field # 7\n",
      "Processing field # 8\n",
      "Processing field # 9\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v13():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(10, activation = 'relu',kernel_regularizer = regularizers.l2(0.001), input_shape = x[1].shape))\n",
    "  model.add(layers.Dense(3, activation = 'softmax'))\n",
    "  model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['acc'])\n",
    "  return model\n",
    "\n",
    "k = 10\n",
    "num_val_samples = len(x_data) // k #integer division\n",
    "print(num_val_samples)\n",
    "num_epochs = 200\n",
    "loss_values = []\n",
    "val_loss_values = []\n",
    "\n",
    "for i in range(k):\n",
    "    print(\"Processing field #\", i)\n",
    "    #prepairing the validation data:data from partition K\n",
    "    val_data = x_data[i*num_val_samples:(i+1)*num_val_samples]      \n",
    "    val_targets = y_data[i*num_val_samples:(i+1)*num_val_samples]\n",
    "    #prepairing training data from all other partitions\n",
    "    partial_train_data = np.concatenate([\n",
    "        x_data[:i*num_val_samples],\n",
    "        x_data[(i+1)*num_val_samples:]\n",
    "    ], axis = 0)\n",
    "    partial_train_target = np.concatenate([\n",
    "        y_data[:i*num_val_samples],\n",
    "        y_data[(i+1)*num_val_samples:]\n",
    "    ],axis = 0)\n",
    "    \n",
    "   \n",
    "    model = model_v13()\n",
    "    history = model.fit(partial_train_data, \n",
    "              partial_train_target,\n",
    "              epochs = num_epochs, \n",
    "              batch_size = 5, verbose = 0,\n",
    "              validation_data = (val_data, val_targets))\n",
    "    history_dict = history.history\n",
    "    loss_values.append(history_dict['loss'])\n",
    "    val_loss_values.append(history_dict['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss_history = [np.mean([x[i] for x in loss_values]) for i in range(num_epochs)]\n",
    "average_val_loss_history = [np.mean([x[i] for x in val_loss_values]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = range(1, len(average_loss_history)+1)\n",
    "plt.plot(epochs, average_loss_history, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, average_val_loss_history, 'b', label = 'Validation loss')\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "10/10 [==============================] - 0s 0us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.12449132651090622, 1.0]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Deep Learning Project 2.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
