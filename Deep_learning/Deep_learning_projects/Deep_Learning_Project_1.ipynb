{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MYDqmMSWmbi5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDB59wjIv3SQ"
   },
   "source": [
    "Next, we can initialize the random number generator to ensure that we always get the same results when executing this code. This will help if we are debugging:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5KF0X-L5nGsT"
   },
   "outputs": [],
   "source": [
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mNHDEZnv5Mf"
   },
   "source": [
    "Now we can load the dataset using pandas and split the columns into 60 input variables (x) and 1 output variable (y. We use pandas to load the data because it easily handles strings (the output variable), whereas attempting to load the data directly using NumPy would be more difficult."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KnnWiBlHnMSJ"
   },
   "outputs": [],
   "source": [
    "# load dataset\n",
    "dataframe = pd.read_csv(\"sonar.csv\", header=None)\n",
    "dataset = dataframe.values\n",
    "# split into input (X) and output (Y) variables\n",
    "x = dataset[:,0:60].astype(float)\n",
    "y = dataset[:,60]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "01ifHd70wDk3"
   },
   "source": [
    "The output variable is string values. We must convert them into integer values 0 and 1.\n",
    "\n",
    "We can do this using the LabelEncoder class from scikit-learn. This class will model the encoding required using the entire dataset via the fit() function, then apply the encoding to create a new output variable using the transform() function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "u3JWACWVnYEy",
    "outputId": "f9e154dc-1693-4f0f-b775-bcde60055af5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['M' 'R']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#converting label to numbers\n",
    "from sklearn import preprocessing\n",
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(np.unique(y))\n",
    "print(le.classes_)\n",
    "y_encoded = le.transform(y)\n",
    "y_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CzXi4UKEwN5O"
   },
   "source": [
    "We can use scikit-learn to evaluate the model using stratified k-fold cross validation. This is a resampling technique that will provide an estimate of the performance of the model. It does this by splitting the data into k-parts, training the model on all parts except one which is held out as a test set to evaluate the performance of the model. This process is repeated k-times and the average score across all constructed models is used as a robust estimate of performance. It is stratified, meaning that it will look at the output values and attempt to balance the number of instances that belong to each class in the k-splits of the data.\n",
    "\n",
    "To use Keras models with scikit-learn, we must use the KerasClassifier wrapper. This class takes a function that creates and returns our neural network model. It also takes arguments that it will pass along to the call to fit() such as the number of epochs and the batch size.\n",
    "\n",
    "Let’s start off by defining the function that creates our baseline model. Our model will have a single fully connected hidden layer with the same number of neurons as input variables. This is a good default starting point when creating neural networks.\n",
    "\n",
    "The weights are initialized using a small Gaussian random number. The Rectifier activation function is used. The output layer contains a single neuron in order to make predictions. It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1 that can easily and automatically be converted to crisp class values.\n",
    "\n",
    "Finally, we are using the logarithmic loss function (binary_crossentropy) during training, the preferred loss function for binary classification problems. The model also uses the efficient Adam optimization algorithm for gradient descent and accuracy metrics will be collected when the model is trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WkWn76xRo5yh"
   },
   "outputs": [],
   "source": [
    "# baseline model\n",
    "def create_baseline():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(60,activation = 'relu',input_shape =(x[1].shape))) #input shape need to be vector so here its (60, )\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89pPOj4cwTIu"
   },
   "source": [
    "Now it is time to evaluate this model using stratified cross validation in the scikit-learn framework.\n",
    "\n",
    "We pass the number of training epochs to the KerasClassifier, again using reasonable default values. Verbose output is also turned off given that the model will be created 10 times for the 10-fold cross validation being performed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7s0vccIpr13w",
    "outputId": "fde2f6c0-3275-4df6-c3c7-27c4541b791f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: 83.71% (6.13%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with standardized dataset\n",
    "estimator = KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(estimator, x, y_encoded, cv=kfold)\n",
    "print(\"Results: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iKrOC-uDwcX0"
   },
   "source": [
    "# Step 3. Re-Run The Baseline Model With Data Preparation\n",
    "It is a good practice to prepare your data before modeling.\n",
    "\n",
    "Neural network models are especially suitable to having consistent input values, both in scale and distribution.\n",
    "\n",
    "An effective data preparation scheme for tabular data when building neural network models is standardization. This is where the data is rescaled such that the mean value for each attribute is 0 and the standard deviation is 1. This preserves Gaussian and Gaussian-like distributions whilst normalizing the central tendencies for each attribute.\n",
    "\n",
    "We can use scikit-learn to perform the standardization of our Sonar dataset using the StandardScaler class:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html \n",
    "\n",
    "Rather than performing the standardization on the entire dataset, it is good practice to train the standardization procedure on the training data within the pass of a cross-validation run and to use the trained standardization to prepare the “unseen” test fold. This makes standardization a step in model preparation in the cross-validation process and it prevents the algorithm having knowledge of “unseen” data during evaluation, knowledge that might be passed from the data preparation scheme like a crisper distribution.\n",
    "\n",
    "We can achieve this in scikit-learn using a Pipeline:\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html \n",
    "\n",
    "The pipeline is a wrapper that executes one or more models within a pass of the cross-validation procedure. Here, we can define a pipeline with the StandardScaler followed by our neural network model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zXWfZEtEsqHo",
    "outputId": "07460b63-b0f2-49d6-b422-bd1955d37d1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Standardized: 85.59% (7.46%)\n"
     ]
    }
   ],
   "source": [
    "# evaluate baseline model with standardized dataset\n",
    "np.random.seed(seed)\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_baseline, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Standardized: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hw3uS1zTwicQ"
   },
   "source": [
    "# Step 4. Tuning Layers and Number of Neurons in The Model\n",
    "There are many things to tune on a neural network, such as the weight initialization, activation functions, optimization procedure and so on.\n",
    "\n",
    "One aspect that may have an outsized effect is the structure of the network itself called the network topology. In this section, we take a look at two experiments on the structure of the network: making it smaller and making it larger.\n",
    "\n",
    "These are good experiments to perform when tuning a neural network on your problem.\n",
    "\n",
    "\n",
    "## 4.1. Evaluate a Smaller Network\n",
    "We suspect that there is a lot of redundancy in the input variables for this project.\n",
    "\n",
    "The data describes the same signal from different angles. Perhaps some of those angles are more relevant than others. We can force a type of feature extraction by the network by restricting the representational space in the first hidden layer.\n",
    "\n",
    "In this experiment, we take our baseline model with 60 neurons in the hidden layer and reduce it by half to 30. This will put pressure on the network during training to pick out the most important structure in the input data to model.\n",
    "\n",
    "We will also standardize the data as in the previous experiment with data preparation and try to take advantage of the small lift in performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PCi0Vz8_txfw",
    "outputId": "a34bcbea-e2f0-4d6a-974f-4f002a678c1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Smaller: 83.52% (8.42%)\n"
     ]
    }
   ],
   "source": [
    "# smaller model\n",
    "np.random.seed(seed)\n",
    "def create_smaller():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(30,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "  \t\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_smaller, epochs=20, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Smaller: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n3bos1pUwqes"
   },
   "source": [
    "## Step 4.2. Evaluate a Larger Network\n",
    "A neural network topology with more layers offers more opportunity for the network to extract key features and recombine them in useful nonlinear ways.\n",
    "\n",
    "We can evaluate whether adding more layers to the network improves the performance easily by making another small tweak to the function used to create our model. Here, we add one new layer (one line) to the network that introduces another hidden layer with 30 neurons after the first hidden layer.\n",
    "\n",
    "Our network now has the topology:\n",
    "\n",
    "60 inputs -> [60 -> 30] -> 1 output\n",
    "\n",
    "The idea here is that the network is given the opportunity to model all input variables before being bottlenecked and forced to halve the representational capacity, much like we did in the experiment above with the smaller network.\n",
    "\n",
    "Instead of squeezing the representation of the inputs themselves, we have an additional hidden layer to aid in the process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "ne1pAUgruv0g",
    "outputId": "f0c4c5ae-a670-45fd-8e38-e28bfe6927d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 84.61% (6.36%)\n"
     ]
    }
   ],
   "source": [
    "# larger model\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_larger():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(60,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(30,activation = 'relu'))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=100, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9ubioDHKw5-f"
   },
   "source": [
    "# Step 5: Really Scaling up: \n",
    "developing a model that overfits\n",
    "Once you’ve obtained a model that has statistical power, the question becomes, is your\n",
    "model sufficiently powerful? Does it have enough layers and parameters to properly\n",
    "model the problem at hand? \n",
    "\n",
    "Remember that the universal tension in machine learning is between optimization and generalization; the ideal model is one that stands right at the border between underfitting and overfitting; between undercapacity and overcapacity. To figure out where this border lies, first you must cross it.\n",
    "\n",
    "To figure out how big a model you’ll need, you must develop a model that overfits.\n",
    "This is fairly easy:\n",
    "1.\tAdd layers.\n",
    "2.\tMake the layers bigger.\n",
    "3.\tTrain for more epochs.\n",
    "\n",
    "Always monitor the training loss and validation loss, as well as the training and validation values for any metrics you care about. When you see that the model’s performance on the validation data begins to degrade, you’ve achieved overfitting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "sAyYV25UxJZm",
    "outputId": "acc4948d-609c-48a0-a1f3-65c1d68c4794"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 87.47% (6.58%)\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_larger():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dense(25,activation = 'relu'))\n",
    "  model.add(layers.Dense(6, activation = 'relu',))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 87.38% (8.12%)\n"
     ]
    }
   ],
   "source": [
    "# with 200 epochs\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_larger_1():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dense(25,activation = 'relu'))\n",
    "  model.add(layers.Dense(6, activation = 'relu',))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger_1, epochs=200, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the performance has started to decrease so we will use the model with 87.95%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MHm0XbrFw7Ba"
   },
   "source": [
    "The next step is to start regularizing and tuning the model, to get as close as possible to the ideal model that neither underfits nor overfits.\n",
    "\n",
    "# Step 6: Tuning the Model\n",
    "With further tuning of aspects like the optimization algorithm etc. and the number of training epochs, it is expected that further improvements are possible. What is the best score that you can achieve on this dataset?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "dwcGzD3-FM7M",
    "outputId": "583bf159-a897-4b2c-9f76-050949c65253"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 87.47% (5.85%)\n"
     ]
    }
   ],
   "source": [
    "# with rmsprop optimizer\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v1():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dense(25,activation = 'relu'))\n",
    "  model.add(layers.Dense(6, activation = 'relu',))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'rmsprop',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v1, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "colab_type": "code",
    "id": "nu_agqOF00al",
    "outputId": "80fe6ed6-5e53-4a02-f2d9-210bae1c091c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 84.07% (5.45%)\n",
      "Larger: 84.07% (5.45%)\n"
     ]
    }
   ],
   "source": [
    "# with adam optimizer and wieght dropout of 20%\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v2():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dropout(0.20))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dropout(0.20))\n",
    "  model.add(layers.Dense(25,activation = 'relu'))\n",
    "  model.add(layers.Dropout(0.20))\n",
    "  model.add(layers.Dense(6, activation = 'relu',))\n",
    "  model.add(layers.Dropout(0.20))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v2, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fB8xoQ_swtR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 82.18% (8.12%)\n"
     ]
    }
   ],
   "source": [
    "# with adam optimizer and wieght dropout of 50%\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v3():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dropout(0.50))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dropout(0.50))\n",
    "  model.add(layers.Dense(25,activation = 'relu'))\n",
    "  model.add(layers.Dropout(0.50))\n",
    "  model.add(layers.Dense(6, activation = 'relu',))\n",
    "  model.add(layers.Dropout(0.50))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v3, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So dropping weights do not produce good results lets try weight regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QmLFaFIXOQDQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 85.99% (6.44%)\n"
     ]
    }
   ],
   "source": [
    "# with L2 weight regularizer \n",
    "from keras import regularizers\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v4():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',kernel_regularizer = regularizers.l2(0.001),input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu',kernel_regularizer = regularizers.l2(0.001)))\n",
    "  model.add(layers.Dense(25,activation = 'relu',kernel_regularizer = regularizers.l2(0.001)))\n",
    "  model.add(layers.Dense(6, activation = 'relu',kernel_regularizer = regularizers.l2(0.001)))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v4, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NSFLokNZQs1E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 83.16% (5.80%)\n"
     ]
    }
   ],
   "source": [
    "# with L1 weight regularizer \n",
    "from keras import regularizers\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v5():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',kernel_regularizer = regularizers.l1(0.001),input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu',kernel_regularizer = regularizers.l1(0.001)))\n",
    "  model.add(layers.Dense(25,activation = 'relu',kernel_regularizer = regularizers.l1(0.001)))\n",
    "  model.add(layers.Dense(6, activation = 'relu',kernel_regularizer = regularizers.l1(0.001)))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v5, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using weight regularizers also do not produce good results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2fDhZ9EYNbnh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 87.45% (5.03%)\n"
     ]
    }
   ],
   "source": [
    "# changing batch size to 1\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "def model_v6():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dense(25,activation = 'relu',))\n",
    "  model.add(layers.Dense(6, activation = 'relu'))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=model_v6, epochs=150, batch_size=1, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 88.90% (6.56%)\n"
     ]
    }
   ],
   "source": [
    "# Using the previous model\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_larger():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dense(25,activation = 'relu'))\n",
    "  model.add(layers.Dense(6, activation = 'relu',))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_larger, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2GUZwAOJw9x3"
   },
   "source": [
    "# Step 7: Rewriting the code using the Keras Functional API\n",
    "Review the April 9, 2018 presentation done by Chollet contained in the project file: \n",
    "\n",
    "Francois_Chollet_March9.pdf\n",
    "\n",
    "Now rewrite the code that you have written so far using the Keras Sequential API in Kearas Functional API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eGLFHTR_F1rv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 88.42% (6.60%)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "def create_model():\n",
    "  \n",
    "  inputs = keras.Input(shape=x[1].shape)\n",
    "  x_1 = layers.Dense(100, activation = 'relu')(inputs)\n",
    "  x_2 = layers.Dense(50, activation = 'relu')(x_1)\n",
    "  x_3 = layers.Dense(25, activation = 'relu')(x_2)\n",
    "  x_4 = layers.Dense(6, activation = 'relu')(x_3)\n",
    "  outputs = layers.Dense(1, activation='sigmoid')(x_4)\n",
    "  model = keras.Model(inputs, outputs)\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ICNPjp8bw_rf"
   },
   "source": [
    "# Step 8: Rewriting the code by doing Model Subclassing\n",
    "Now rewrite the code that you have written so far using the Keras Model Subclassing as mentioned in the Chollet April 9, 2018 presentation.\n",
    "\n",
    "Reference:\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/Model\n",
    "Please note you will have to use TensorFlow 1.7+ with built-in Keras. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OqNlABaTL6dQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Larger: 87.92% (6.95%)\n"
     ]
    }
   ],
   "source": [
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "class MyModel(keras.Model):\n",
    "  def __init__(self):\n",
    "    super(MyModel, self).__init__()\n",
    "    self.dense1 = layers.Dense(100, activation = 'relu')\n",
    "    \n",
    "    self.dense2 = layers.Dense(50, activation = 'relu')\n",
    "    self.dense3 = layers.Dense(25, activation = 'relu')\n",
    "    self.dense4 = layers.Dense(6, activation = 'relu')\n",
    "    self.dense5 = layers.Dense(1, activation = 'sigmoid')\n",
    "    \n",
    "  def call(self, inputs):\n",
    "    x_1 = self.dense1(inputs)\n",
    "    x_2 = self.dense2(x_1)\n",
    "    x_3 = self.dense3(x_2)\n",
    "    x_4 = self.dense4(x_3)\n",
    "    return self.dense5(x_4)\n",
    "  \n",
    "def create_model():\n",
    "  model = MyModel()\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "\n",
    "estimators = []\n",
    "estimators.append(('standardize', StandardScaler()))\n",
    "estimators.append(('mlp', KerasClassifier(build_fn=create_model, epochs=150, batch_size=5, verbose=0)))\n",
    "pipeline = Pipeline(estimators)\n",
    "kfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=seed)\n",
    "results = cross_val_score(pipeline, x, y_encoded, cv=kfold)\n",
    "print(\"Larger: %.2f%% (%.2f%%)\" % (results.mean()*100, results.std()*100))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JFgXHhimxCQH"
   },
   "source": [
    "# Step 9: Rewriting the code without using scikit-learn\n",
    "Once you have written the model in all three API style you are required to do k-fold cross validation without using scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0gH6sK5kQnyY"
   },
   "outputs": [],
   "source": [
    "np.random.seed(seed)\n",
    "dataset = dataframe.values\n",
    "np.random.shuffle(dataset)\n",
    "# split into input (X) and output (Y) variables\n",
    "x = dataset[:,0:60].astype(float)\n",
    "y = dataset[:,60]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 1, 1,\n",
       "       0, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 0, 0, 1, 0, 0, 1, 1, 0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_encoded = []\n",
    "for value in y:\n",
    "    if value=='R':\n",
    "        y_encoded.append(1)\n",
    "    else:\n",
    "        y_encoded.append(0)\n",
    "        \n",
    "y_encoded = np.asarray(y_encoded)\n",
    "y_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "InAvFlykQp2f",
    "outputId": "00d84263-877b-4159-a8d1-7e84e536d6bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Processing field # 0\n",
      "20/20 [==============================] - 1s 71ms/step\n",
      "Processing field # 1\n",
      "20/20 [==============================] - 1s 70ms/step\n",
      "Processing field # 2\n",
      "20/20 [==============================] - 2s 78ms/step\n",
      "Processing field # 3\n",
      "20/20 [==============================] - 1s 74ms/step\n",
      "Processing field # 4\n",
      "20/20 [==============================] - 1s 74ms/step\n",
      "Processing field # 5\n",
      "20/20 [==============================] - 2s 76ms/step\n",
      "Processing field # 6\n",
      "20/20 [==============================] - 2s 83ms/step\n",
      "Processing field # 7\n",
      "20/20 [==============================] - 2s 82ms/step\n",
      "Processing field # 8\n",
      "20/20 [==============================] - 2s 86ms/step\n",
      "Processing field # 9\n",
      "20/20 [==============================] - 2s 121ms/step\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "def create_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dense(25,activation = 'relu'))\n",
    "  model.add(layers.Dense(6, activation = 'relu',))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "\n",
    "k = 10\n",
    "num_val_samples = len(x) // k #integer division\n",
    "print(num_val_samples)\n",
    "num_epochs = 10\n",
    "results = []\n",
    "for i in range(k):\n",
    "    print(\"Processing field #\", i)\n",
    "    #prepairing the validation data:data from partition K\n",
    "    val_data = x[i*num_val_samples:(i+1)*num_val_samples]      \n",
    "    val_targets = y_encoded[i*num_val_samples:(i+1)*num_val_samples]\n",
    "    #prepairing training data from all other partitions\n",
    "    partial_train_data = np.concatenate([\n",
    "        x[:i*num_val_samples],\n",
    "        x[(i+1)*num_val_samples:]\n",
    "    ], axis = 0)\n",
    "    partial_train_target = np.concatenate([\n",
    "        y_encoded[:i*num_val_samples],\n",
    "        y_encoded[(i+1)*num_val_samples:]\n",
    "    ],axis = 0)\n",
    "    partial_train_data_mean = partial_train_data.mean(axis = 0)\n",
    "    partial_train_data_std = partial_train_data.std(axis = 0)\n",
    "    partial_train_data = (partial_train_data - partial_train_data_mean)/partial_train_data_std\n",
    "    val_data = (val_data - partial_train_data_mean)/partial_train_data_std\n",
    "   \n",
    "    model = create_model()\n",
    "    model.fit(partial_train_data, \n",
    "              partial_train_target,\n",
    "              epochs = num_epochs, \n",
    "              batch_size = 5, verbose = 0)\n",
    "    results.append(model.evaluate(val_data, val_targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B1iXi1fKQ221"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "colab_type": "code",
    "id": "QPlFI84MQ6uO",
    "outputId": "2cb4e9c2-2f9c-40ac-ab54-af2194ee9d55"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "87.99999952316284"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = []\n",
    "for result in results:\n",
    "    acc.append(result[1])\n",
    "acc = np.asarray(acc)\n",
    "overall_acc = acc.mean()\n",
    "overall_acc*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Exfp2XSOgv8S",
    "outputId": "b513fdbf-ebc8-4fab-df96-2804b98f4261"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loss', 'acc']"
      ]
     },
     "execution_count": 55,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "id": "InAvFlykQp2f",
    "outputId": "00d84263-877b-4159-a8d1-7e84e536d6bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "Processing field # 0\n",
      "Processing field # 1\n",
      "Processing field # 2\n",
      "Processing field # 3\n",
      "Processing field # 4\n",
      "Processing field # 5\n",
      "Processing field # 6\n",
      "Processing field # 7\n",
      "Processing field # 8\n",
      "Processing field # 9\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(seed)\n",
    "def create_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Dense(100,activation = 'relu',input_shape =(x[1].shape)))\n",
    "  model.add(layers.Dense(50,activation = 'relu'))\n",
    "  model.add(layers.Dense(25,activation = 'relu'))\n",
    "  model.add(layers.Dense(6, activation = 'relu',))\n",
    "  model.add(layers.Dense(1, activation = 'sigmoid'))\n",
    "  model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics=['accuracy'] )\n",
    "  return model\n",
    "\n",
    "k = 10\n",
    "num_val_samples = len(x) // k #integer division\n",
    "print(num_val_samples)\n",
    "num_epochs = 200\n",
    "results = []\n",
    "loss_values = []\n",
    "val_loss_values = []\n",
    "for i in range(k):\n",
    "    print(\"Processing field #\", i)\n",
    "    #prepairing the validation data:data from partition K\n",
    "    val_data = x[i*num_val_samples:(i+1)*num_val_samples]      \n",
    "    val_targets = y_encoded[i*num_val_samples:(i+1)*num_val_samples]\n",
    "    #prepairing training data from all other partitions\n",
    "    partial_train_data = np.concatenate([\n",
    "        x[:i*num_val_samples],\n",
    "        x[(i+1)*num_val_samples:]\n",
    "    ], axis = 0)\n",
    "    partial_train_target = np.concatenate([\n",
    "        y_encoded[:i*num_val_samples],\n",
    "        y_encoded[(i+1)*num_val_samples:]\n",
    "    ],axis = 0)\n",
    "    partial_train_data_mean = partial_train_data.mean(axis = 0)\n",
    "    partial_train_data_std = partial_train_data.std(axis = 0)\n",
    "    partial_train_data = (partial_train_data - partial_train_data_mean)/partial_train_data_std\n",
    "    val_data = (val_data - partial_train_data_mean)/partial_train_data_std\n",
    "   \n",
    "    model = create_model()\n",
    "    history = model.fit(partial_train_data, \n",
    "              partial_train_target,\n",
    "              epochs = num_epochs, \n",
    "              batch_size = 5, verbose = 0,\n",
    "              validation_data = (val_data, val_targets))\n",
    "    history_dict = history.history\n",
    "    loss_values.append(history_dict['loss'])\n",
    "    val_loss_values.append(history_dict['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_loss_history = [np.mean([x[i] for x in loss_values]) for i in range(num_epochs)]\n",
    "average_val_loss_history = [np.mean([x[i] for x in val_loss_values]) for i in range(num_epochs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "epochs = range(1, len(average_loss_history)+1)\n",
    "plt.plot(epochs, average_loss_history, 'bo', label = 'Training loss')\n",
    "plt.plot(epochs, average_val_loss_history, 'b', label = 'Validation loss')\n",
    "plt.title(\"Training and Validation loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Deep Learning Project 1.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
